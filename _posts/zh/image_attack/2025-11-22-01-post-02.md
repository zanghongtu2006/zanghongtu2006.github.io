---
layout: post
lang: zh
title: "第二章 · 对抗样本的数学基础：梯度、范数与最小扰动"
date: "2025-11-24 23:28:47"
slug: "ai-attack-and-secure-02"
translations:
  en: /en/ai-attack-and-secure-02/
  de: /de/ai-attack-and-secure-02/
categories: ["AI"]
tags: ["AI", "ATTACK"]
---

# 第二章 · 对抗样本的数学基础：梯度、范数与最小扰动

*理解对抗样本，必须从“数学结构”层面入手。为什么极微小的扰动能欺骗模型？为什么机器学习模型对高维输入如此脆弱？本章将从梯度、Lp 范数、最小扰动理论等关键概念入手，为后续章节的攻击算法铺路。*

---

#  1. 什么是对抗扰动（Adversarial Perturbation）

给定一个输入图像 \( x \) 和一个训练好的分类模型 \( f(\cdot) \)：

我们希望找到一个 **对人眼不可察觉但能误导模型的扰动**：

\[
x' = x + \delta
\]

使模型预测错误：

\[
f(x') \ne f(x)
\]

并且扰动极小：

\[
\|\delta\|_p \le \epsilon
\]

其中：

- \( \delta \)：对抗扰动
- \( \epsilon \)：扰动限制（非常小，如 \(8/255\)）
- \( \|\cdot\|_p \)：Lp 范数

这个数学式子就是对抗攻击的核心。

---

#  2. 梯度为何能“制造错觉”？

深度模型的预测依赖于输入的 **梯度敏感性**。

对于一个分类模型，其 loss 为：

\[
L(f(x), y)
\]

如果对输入求梯度：

\[
\nabla_x L
\]

它表示：  
> **如何改变输入能让模型损失最大、最不稳定。**

对抗攻击的核心思想：

> 顺着梯度方向，让模型“痛点最大化”，就能制造对抗样本。

---

## ▶ 例子：FGSM（最基础的攻击）  
使用符号梯度：

\[
x' = x + \epsilon \cdot \text{sign}(\nabla_x L)
\]

只需一次梯度计算，就能让模型完全错误。

---

#  3. 为什么“微小扰动”在人眼不可见？

人类的视觉不是按像素独立看的，而是做模式识别。

但深度神经网络计算的是：

> 高维输入上的线性近似。

比如对于一张 224×224×3 的 RGB 图像，维度是：

\[
224 \times 224 \times 3 = 150,528\text{ 维}
\]

在如此高维空间中：

- 微小扰动在每个维度都极小  
- 但它们叠加的影响可能巨大

数学上：

\[
w^T(x + \delta) = w^T x + w^T \delta
\]

即使 \(\delta\) 看起来很小，  
但内积 \( w^T\delta \) 会在高维空间中被 **成千上万个维度放大**。

---

#  4. Lp 范数的作用：衡量扰动“大小”的方式

对抗攻击中常见的三种范数：

---

## ### **1）L∞ 范数（最常用）**
\[
\|\delta\|_\infty = \max_i |\delta_i|
\]

限制每个像素最大变化量，例如：

- \( \epsilon = 8/255 \)
  - 每个像素最多改动 3% 的灰度  
  - 人类几乎看不见  
  - 却能让模型完全失效

**绝大多数对抗攻击都是 L∞ 的。**

---

## ### **2）L2 范数（欧几里得距离）**
\[
\|\delta\|_2 = \sqrt{\sum_i \delta_i^2}
\]

优势：

- 允许每个像素变化更自然
- 图像更加真实

经典攻击：C&W（Carlini & Wagner）

---

## ### **3）L0 范数（改动的像素数量）**
\[
\|\delta\|_0 = \text{number of non-zero pixels}
\]

只改动少量像素，但改动幅度大。

常用于：

- patch 攻击
- 改小块区域
- 人脸眼镜贴纸攻击

---

#  5. 最小扰动问题（Optimization View）

对抗攻击本质上是一个 **优化问题**：

\[
\text{minimize } \|\delta\|_p
\]

\[
\text{subject to } f(x+\delta) \ne y
\]

这是一个困难的非凸优化问题。

不同攻击方法，就是用不同的方法近似求解这个问题：

- FGSM：一次梯度更新
- BIM / PGD：多步梯度下降
- C&W：直接优化扰动的数值
- NES / ZOO：黑盒估计梯度
- HSJ / Boundary：不看梯度，用几何逼近

---

#  6. 为什么“最小扰动”是对抗攻击成功的关键？

如果扰动太大，人眼会看到。  
但最小扰动有三个优势：

### ✔ **隐蔽性强**：难以人工检查  
### ✔ **迁移性强**：能在不同模型上成功  
### ✔ **更难检测**：传统去噪方法难以防御  

许多攻击甚至可以把扰动缩在 **1/255** 量级以下。

---

#  7. Loss Landscape：对抗攻击的“地形图”

可以把模型的输出看作一个地形：

- “山谷”代表正确预测
- “山脊”代表错误预测区域

模型往往在山谷中非常狭窄：

> 稍微推动一下，就会掉进山谷外。

对抗扰动就是在高维地形上“轻轻一推”，  
就能让模型掉进错误区域。

我们后续章节会展示不同攻击如何利用这个地形结构。

---

#  8. 小结：本章的关键要点

- 对抗样本的数学定义：  
  \[
  x' = x + \delta, \quad \|\delta\|_p \le \epsilon
  \]
- 梯度是寻找脆弱方向的关键
- 高维空间让“微扰动”产生巨大影响
- Lp 范数定义扰动大小
- 最小扰动是优化问题
- 对抗攻击的核心是：  
  **利用模型的高维脆弱性 + 梯度敏感性**

本章奠定了攻击算法理解的基础。  
下一章我们将深入白盒攻击，  
从 FGSM、BIM、PGD 到 C&W、AutoAttack，  
全面解构它们的数学原理。

---
